---
title: "Authentic vs Fake Banknote Classification"
author: "Willian Ho, Sayana Imash, Danny Pirouz, & Arad Sabet"
format: html
editor: source
execute:
  echo: false
toc: true
toc-depth: 4
bibliography: references.bib
---
```{python}
import pandas as pd
from IPython.display import Markdown, display
from tabulate import tabulate

analysis_results = "../results/model_results/"
eda_results = "../results/eda/"
```

## Summary

### Background information

Counterfeit currency is a global challenge that poses great threat to financial institutions, governments, and even individuals. To combat this, central banks and law enforcement agencies invest billions of dollars annually in anti-counterfeiting measures, such as watermarks, holograms, and security threads. However, while these methods help deter fraud, rapid advancements in printing technology and reproduction techniques have made counterfeiting even more complex and harder to detect [@US_Banknote_Counterfeits].

In Canada, for example, the Department of Justice reports that counterfeit money is one of the most common forms of financial fraud that is continuously adapting to improved security features and law enforcement tactics [@Canada_Fraud]. This ongoing challenge highlights the urgent need for more advanced detection methods that can quickly and accurately identify fraudulent banknotes. One of the most promising solutions is the use of automated detection systems powered by image processing and machine learning. By improving pattern recognition and statistical analysis, these models provide high precision, speed, and efficiency, making them a powerful tool in combating financial fraud [@Fraud_Prevention].

### Research Question

In this project we are trying to answer the question whether a machine learning model, specifically K-Nearest Neighbors (KNN), can effectively classify banknotes as genuine or counterfeit based on numerical image features. 

### The dataset

The dataset we used for this project is called "Banknote Authentication UCI data", and it was taken from the UCI Machine Learning Repository. This dataset is designed to distinguish between genuine and counterfeit banknotes using features extracted from their images. It comprises of 1372 individual data points that are characterized by four continuous features: variance, skewness, curtosis, entropy, and class. The target variable "class" is a binary value of 1 (counterfeit banknote) or 0 (genuine banknote). These features were extracted using wavelet transform tools applied to 400x400 pixel grayscale images captured by an industrial camera [@Banknote_Authentication]. 

## Methods

The Python programming language [@Python]
and the following Python packages were used to perform the analysis: 

* pandas [@pandas]
* matplotlib [@matplotlib]
* seaborn [@seaborn]
* Quarto [@Allaire_Quarto_2022]
* click [@click]

## Results

### Exploratory Data Analysis & Visualizations
```{python}
#| label: tbl-count
#| tbl-cap: Distribution of Banknote Authentication Notes
count_table = pd.read_csv(f"{eda_results}BankNote_Authentication_EDA_count_table.csv")
Markdown(count_table.to_markdown(index = False))
```
The count between classes in @tbl-count is fairly evenly split. Since there's not a drastic difference between the distribution of classes, we will not do any weighting.

![Distribution of Entropy Grouped by Authenticity](../results/eda/BankNote_Authentication_EDA_entropy.png){#fig-entropy width=90%}

The entropy histograms in @fig-entropy have very similar distributions for authentic and fake bills as they both have their modes around the same value and generally follow the same patterns. Since the distributions are very similar, it is very unlikely that entropy is a driving factor in determining authentic or fake bills. 

![Distribution of Curtosis Grouped by Authenticity](../results/eda/BankNote_Authentication_EDA_curtosis.png){#fig-curtosis width=90%}

The same can be said for the curtosis histogram (@fig-curtosis) as authentic and fake bills have similar distributions with their modes around the same value as well. Curtosis is also unlikely to be a driving force. 

![Distribution of Variance Grouped by Authenticity](../results/eda/BankNote_Authentication_EDA_variance.png){#fig-variance width=90%}

The variance distributions (@fig-variance) are very different. Both authentic and fake bills have a bell curve distribution with a similar pattern in terms of distribution, but the position where the distribution occurs differs drastically. This suggests that variance is a very strong driving force in determining whether a bill is authentic or fake. 

![Distribution of Skewness Grouped by Authenticity](../results/eda/BankNote_Authentication_EDA_skewness.png){#fig-skewness width=90%}

The skewness histograms (@fig-skewness) have somewhat different distributions. Fake bills tend to have lower skewness and the mode is noticeably lower for the fake bills compared to the real bills, but the distributions still overlap quite a bit suggesting that skewness is not as big of a driving factor as variance. These histograms will allow us to understand what we should expect with our unknown bill once the bill is identified. We can use these histograms to see if our expectations line up with the unknown bill.

```{python}
#| label: tbl-p_values
#| tbl-cap: P-values for the explanatory variables
#| 
p_value = 0.05
p_value_percentage = p_value * 100
p_values = pd.read_csv(f"{eda_results}BankNote_Authentication_EDA_p_values.csv")
Markdown(p_values.to_markdown(index = False))
```

For our hypothesis test, the null hypothesis assumes that there is no effect or significant differences between the two classes. We will set the default so that p-values below `{python} p_value` suggest strong evidence against the null hypothesis. Based on @tbl-p_values, variance, entropy, and curtosis have p-values below 0.05. We can reject the null hypothesis at a `{python} p_value_percentage`% significance level, meaning that all of variance entropy, and curtosis are all likely significant in predicting the authenticity of a banknote. The skewness is the only value way above the `{python} p_value` p-value threshold. This suggests that skewness isn't significant in predicting the authenticity of a bank note, while the other 3 variables are.

### Data Analysis: Creation of the KNN Model

#### Cross validation

We tried a standard scaler but got better results without it. We decided to use a KNN Model without any scaling.

#### Parameter optimization
```{python}
KNN_parameters = pd.read_csv(f"{analysis_results}BankNote_Authentication_mr_KNN_parameters.csv")
best_k = int(KNN_parameters['best_k'].iloc[0])
```

![KNN Accuracy vs Number of Neighbours](../results/model_results/BankNote_Authentication_mr_knn_cv.png){#fig-neighbours width=90%}

We will run our KNN model on the earliest number of neighbours that results in the highest accuracy on the training set. @fig-neighbours shows how the number of neighbours affects the accuracy on the training set. Based on this graph we will use `{python} best_k` neighbours, as it results in the highest training accuracy. This means we can start by experimenting with `{python} best_k` neighbours in our pipeline and try it on the test set.

#### Results on the Testing Set

```{python}
count_table = pd.read_csv(f"{analysis_results}BankNote_Authentication_mr_classification_report.csv", index_col=0, dtype={'f1-score': 'float', 'support': 'int'})
test_accuracy = float(count_table.loc[['accuracy'], 'f1-score'].iloc[0] * 100)
positive_cases = int(count_table.loc[['0'], 'support'].iloc[0])
negative_cases = int(count_table.loc[['1'], 'support'].iloc[0])
total_cases = positive_cases + negative_cases
```
![Confusion Matrix of the KNN Model on the Test Set](../results/model_results/BankNote_Authentication_mr_confusion_matrix.png){#fig-confusion_matrix width=90%}

Based on our confusion matrix from @fig-confusion_matrix, we were able to predict `{python} positive_cases` authentic banknotes and `{python} negative_cases` fake banknotes correctly. 
```{python}
#| label: tbl-classification_report
#| tbl-cap: Classification Report of the KNN Model on the Test Set
Markdown(count_table.to_markdown())
```

@tbl-classification_report further shows this conclusion. We achieved a score of `{python} test_accuracy`%. This means that we have no error, and we are predicting everything right. Every test sample was classified correctly. There are no misclassifications, which implies that the model perfectly distinguishes between the two classes. In conclusion, the classifier achieved **`{python} test_accuracy`% accuracy** on the test set, with perfect precision, recall, and F1-scores for both classes. 

![SHAP plot for Authentic Banknote](../results/model_results/BankNote_Authentication_mr_shap_plot_1.png){#fig-shap_1 width=90%}

![SHAP plot for Fake Banknote](../results/model_results/BankNote_Authentication_mr_shap_plot_2.png){#fig-shap_2 width=90%}

@fig-shap_1 and @fig-shap_2 show an example of how impactful each explanatory variable was to predict the class via a SHAP plot. In both cases, variance is the most important indicator of the class while entropy is the least important.

## Discussion
### Analysis of Results

The classifier perfectly predicted all `{python} positive_cases` real banknotes and `{python} negative_cases` fake banknotes in the test set. This is not what we expected to find. While we did expect our model to do well since our input variables likely play a major role in detecting forged banknotes, we did not expect it to perfectly predict every banknote in the test set.

However, upon further research, this model is fairly consistent with other models. A Gaussian process classifier was able to get 100% accuracy on both the training and test set with only 500 training samples [@Gaussian_Process_Classifier]. A different model using convex optimzation on graphs also managed to achieve an average testing accuracy of 99.03% over 100 randomly generated training sets [@Graph_Optimization]. 

### Future Considerations 

This result suggests that our current model is identifying all the patterns in the dataset and is able to predict whether or not a banknote is forged or not with `{python} test_accuracy`% accuracy. However, this model still needs more testing. We could be overfitting on our available data or have a data leakage. We added a data validation check that detects anomalous correlations between the target variable compared to the explanatory variables to minimize potential data leakage, but this may not be enough to prevent it. In terms of overfitting, our test set may also not capture all the patterns of a banknote the model might encounter, as it only has `{python} total_cases` samples. This test could have failed to include some rare or outlier patterns the model may encounter if it were tested on a different dataset. It is also possible that the test set may be contaminated in a way that was not accounted for, leading the model to have already seen the test set during training. It may even be the case that our entire dataset is too small of a sample, which makes the dataset not be representative of the real world. This could potentially cause our model to perform poorly on unseen data. 

This analysis raises several potential future questions that could evaluate how this model would fare in real world scenarios:

* Would our model still be generalizable for larger or more complex datasets or would it lead to lower accuracy?
* Which of the input features between variance, skewness, kurtosis, and entropy are the most important for authenticating banknotes and by how much?**
* Is there covariance between the input features? Does this covariance make it easier to detect fake banknotes?**
* What are the patterns that make a counterfeit banknote stand out from a genuine banknote? Could a forged banknote be designed to avoid these patterns and not be detected by the classification model?

## References